{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with PyTorch\n",
    "\n",
    "In this notebook, you'll get introduced to [PyTorch](http://pytorch.org/), a framework for building and training neural networks. PyTorch in a lot of ways behaves like the arrays you love from Numpy. These Numpy arrays, after all, are just tensors. PyTorch takes these tensors and makes it simple to move them to GPUs for the faster processing needed when training neural networks. It also provides a module that automatically calculates gradients (for backpropagation!) and another module specifically for building neural networks. All together, PyTorch ends up being more coherent with Python and the Numpy/Scipy stack compared to TensorFlow and other frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install PyTorch\n",
    "\n",
    "Getting PyTorch up and running is straightforward. Use the following command based on your system configuration [Start Locally](https://pytorch.org/get-started/locally/):\n",
    "\n",
    "<img src=\"assets\\Capture.JPG\" alt=\"Example Image\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CPU-only\n",
    "!pip install torch torchvision\n",
    "\n",
    "# For GPU support (make sure you have CUDA installed)\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 14 19:23:56 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.85                 Driver Version: 555.85         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650      WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   46C    P8              2W /   35W |    1611MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     12840      C   C:\\Users\\lenovo\\anaconda3\\python.exe        N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check your CUDA driver and device.\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start by importing PyTorch and checking the version we're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0+cu124'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Tensors\n",
    "\n",
    "It turns out neural network computations are just a bunch of linear algebra operations on *tensors*, a generalization of matrices. A vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor (RGB color images for example). The fundamental data structure for neural networks are tensors and PyTorch (as well as pretty much every other deep learning framework) is built around tensors.\n",
    "\n",
    "<img src=\"assets\\tensor_examples.svg\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing a Tensor\n",
    "\n",
    "Tensors can be initialized in various ways. Take a look at the following examples:\n",
    "\n",
    "**Directly from data**\n",
    "\n",
    "Tensors can be created directly from data. The data type is automatically inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From a NumPy array**\n",
    "\n",
    "Tensors can be created from NumPy arrays using `torch.from_numpy` (and vice versa - see `torch.tensor.numpy`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From another tensor:**\n",
    "\n",
    "The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.7710, 0.3980],\n",
      "        [0.8417, 0.7372]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Tensors (from dimensionality prespective)\n",
    "\n",
    "<img src='assets\\Tensor_types.jpeg' width=600px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank 0 Tensor **(scalar)**\n",
    "\n",
    "A scalar is a single number and in tensor-speak it's a zero dimension tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar\n",
    "scalar = torch.tensor(7)\n",
    "scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to retrieve the number from the tensor? To do we can use the `item()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the Python number within a tensor (only works with one-element tensors)\n",
    "scalar.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the dimensions of a tensor using the `ndim` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank 1 Tensor **(Vector)**\n",
    "\n",
    "A vector is a single dimension tensor but can contain many numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector\n",
    "vector = torch.tensor([7, 7])\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many dimensions do you think it'll have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of dimensions of vector\n",
    "vector.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, `vector` contains two numbers but only has a single dimension.\n",
    "\n",
    "You can tell the number of dimensions a tensor in PyTorch has by the number of square brackets on the outside ([) and you only need to count one side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dimension VS Shape**\n",
    "\n",
    "Another important concept for tensors is their shape attribute. The shape tells you how the elements inside them are arranged. For vectors, the shape will have only one number which tells you the length of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of vector\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above returns `torch.Size([2])` which means our vector has a shape of `[2]`. This is because of the two elements we placed inside the square brackets `([7, 7])`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank 2 Tensor **(Matrix)**\n",
    "\n",
    "A vector is a single dimension tensor but can contain many numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  8],\n",
       "        [ 9, 10]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix\n",
    "MATRIX = torch.tensor([[7, 8], \n",
    "                        [9, 10]])\n",
    "MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of dimensions\n",
    "MATRIX.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MATRIX` has two dimensions (did you count the number of square brackets on the outside of one side?).\n",
    "\n",
    "What `shape` do you think it will have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MATRIX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank `N` Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [3, 6, 9],\n",
       "         [2, 4, 5]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor\n",
    "TENSOR = torch.tensor([[[1, 2, 3],\n",
    "                        [3, 6, 9],\n",
    "                        [2, 4, 5]]])\n",
    "TENSOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many dimensions do you think it has? (hint: use the square bracket counting trick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of dimensions for TENSOR\n",
    "TENSOR.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what about its shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of TENSOR\n",
    "TENSOR.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, it outputs `torch.Size([1, 3, 3])`.\n",
    "\n",
    "The dimensions go outer to inner.\n",
    "\n",
    "That means there's 1 dimension of 3 by 3.\n",
    "\n",
    "<img src=\"assets/PyTorch-different-tensor-dimensions.png\" alt=\"Example Image\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of a Tensor\n",
    "\n",
    "Tensor attributes describe their shape, datatype, and the device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will see later that we can perform operations using this tensors, Each of these operations can be run on the GPU (at typically higher speeds than on a CPU). If you’re using Colab, allocate a GPU by going to `Runtime > Change runtime type > GPU`.\n",
    "\n",
    "By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using `.to` method (after checking for GPU availability). Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device tensor is stored on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")\n",
    "    \n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on Tensors\n",
    "\n",
    "Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are comprehensively described [here](https://pytorch.org/docs/stable/torch.html).\n",
    "\n",
    "Let's dive into Tensors operations\n",
    "- **Arithmetic Operations**: Addition, subtraction, multiplication, division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors\n",
    "a = torch.tensor([2.0, 4.0, 6.0])\n",
    "b = torch.tensor([1.0, 3.0, 5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition:\n",
      "tensor([ 3.,  7., 11.])\n",
      "\n",
      "Subtraction:\n",
      "tensor([1., 1., 1.])\n",
      "\n",
      "Element-wise Multiplication:\n",
      "tensor([ 2., 12., 30.])\n",
      "\n",
      "Element-wise Division:\n",
      "tensor([2.0000, 1.3333, 1.2000])\n"
     ]
    }
   ],
   "source": [
    "# Arithmetic Operations\n",
    "print(\"Addition:\")\n",
    "print(a + b)\n",
    "\n",
    "print(\"\\nSubtraction:\")\n",
    "print(a - b)\n",
    "\n",
    "print(\"\\nElement-wise Multiplication:\")\n",
    "print(a * b)\n",
    "\n",
    "print(\"\\nElement-wise Division:\")\n",
    "print(a / b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **In-place Operations**: Operations that modify tensors without allocating new memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In-place Multiplication (a *= 2):\n",
      "tensor([ 4.,  8., 12.])\n"
     ]
    }
   ],
   "source": [
    "# In-place Operations\n",
    "print(\"\\nIn-place Multiplication (a *= 2):\")\n",
    "a.mul_(2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Reduction Operations**: Summing, averaging, or finding the maximum of tensor elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sum of elements in c:\n",
      "tensor(10.)\n",
      "\n",
      "Mean of elements in c:\n",
      "tensor(2.5000)\n",
      "\n",
      "Maximum value in c:\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# Reduction Operations\n",
    "c = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "print(\"\\nSum of elements in c:\")\n",
    "print(c.sum())\n",
    "\n",
    "print(\"\\nMean of elements in c:\")\n",
    "print(c.mean())  # Convert to float before calling mean()\n",
    "\n",
    "print(\"\\nMaximum value in c:\")\n",
    "print(c.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Matrix Multiplication**: Performing dot products and matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrix Multiplication of c and d:\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix Multiplication\n",
    "d = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\n",
    "print(\"\\nMatrix Multiplication of c and d:\")\n",
    "print(torch.matmul(c, d))  # or torch.mm(c, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Slicing\n",
    "\n",
    "Indexing and slicing in PyTorch allow you to access and manipulate specific elements, rows, columns, or slices of tensors. This is essential for data preprocessing and manipulation in deep learning models.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- **Basic Indexing**: Accessing individual elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "tensor([[10, 20, 30],\n",
      "        [40, 50, 60],\n",
      "        [70, 80, 90]])\n"
     ]
    }
   ],
   "source": [
    "# Create a 2D tensor\n",
    "tensor = torch.tensor([[10, 20, 30],\n",
    "                       [40, 50, 60],\n",
    "                       [70, 80, 90]])\n",
    "print(\"Original Tensor:\")\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Element at row 1, column 2:\n",
      "tensor(60)\n"
     ]
    }
   ],
   "source": [
    "# Basic Indexing\n",
    "print(\"\\nElement at row 1, column 2:\")\n",
    "print(tensor[1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Slicing**: Extracting sub-tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First two rows and last two columns:\n",
      "tensor([[20, 30],\n",
      "        [50, 60]])\n"
     ]
    }
   ],
   "source": [
    "# Slicing\n",
    "print(\"\\nFirst two rows and last two columns:\")\n",
    "print(tensor[:2, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Integer Array Indexing**: Using tensors of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elements at positions (0,1) and (2,0):\n",
      "tensor([20, 70])\n"
     ]
    }
   ],
   "source": [
    "# Integer Array Indexing\n",
    "rows = torch.tensor([0, 2])\n",
    "cols = torch.tensor([1, 0])\n",
    "print(\"\\nElements at positions (0,1) and (2,0):\")\n",
    "print(tensor[rows, cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Boolean Masking**: Filtering elements based on conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elements greater than or equal to 50:\n",
      "tensor([50, 60, 70, 80, 90])\n"
     ]
    }
   ],
   "source": [
    "# Boolean Masking\n",
    "print(\"\\nElements greater than or equal to 50:\")\n",
    "print(tensor[tensor >= 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to Other Python Objects\n",
    "\n",
    "Interoperability with other Python libraries is often necessary. PyTorch tensors can be converted to NumPy arrays, Python scalars, and lists.\n",
    "\n",
    "Examples and Improvements\n",
    "- **Tensor to NumPy Array**: Using `tensor.numpy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy array: [100 200 300]\n"
     ]
    }
   ],
   "source": [
    "# Tensor to NumPy array\n",
    "tensor = torch.tensor([100, 200, 300])\n",
    "array = tensor.numpy()\n",
    "print(\"NumPy array:\", array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pay Attention**\n",
    "\n",
    "the values in the array will be the same as the tensor and if you change the values in the array, it will change the values of the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modifying array[0] = 999\n",
      "NumPy array: [999 200 300]\n",
      "Tensor: tensor([999, 200, 300])\n"
     ]
    }
   ],
   "source": [
    "# Modifying the NumPy array affects the tensor\n",
    "array[0] = 999\n",
    "print(\"\\nAfter modifying array[0] = 999\")\n",
    "print(\"NumPy array:\", array)\n",
    "print(\"Tensor:\", tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **NumPy Array to Tensor**: Using `torch.from_numpy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensor from NumPy array: tensor([400, 500, 600], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# NumPy array to Tensor\n",
    "np_array = np.array([400, 500, 600])\n",
    "tensor_from_array = torch.from_numpy(np_array)\n",
    "print(\"\\nTensor from NumPy array:\", tensor_from_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Sharing Memory**: Understanding that they share underlying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modifying tensor_from_array[1] = 555\n",
      "Tensor: tensor([400, 555, 600], dtype=torch.int32)\n",
      "NumPy array: [400 555 600]\n"
     ]
    }
   ],
   "source": [
    "# Modifying the tensor affects the NumPy array\n",
    "tensor_from_array[1] = 555\n",
    "print(\"\\nAfter modifying tensor_from_array[1] = 555\")\n",
    "print(\"Tensor:\", tensor_from_array)\n",
    "print(\"NumPy array:\", np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Converting to Scalars and Lists**: Using `item()` and `tolist()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python scalar: 42\n"
     ]
    }
   ],
   "source": [
    "# Tensor to Python scalar\n",
    "scalar_tensor = torch.tensor(42)\n",
    "scalar_value = scalar_tensor.item()\n",
    "print(\"\\nPython scalar:\", scalar_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List from tensor: [[1, 2], [3, 4]]\n"
     ]
    }
   ],
   "source": [
    "# Tensor to list\n",
    "tensor = torch.tensor([[1, 2], [3, 4]]) # 2D Tensor\n",
    "list_from_tensor = tensor.tolist()\n",
    "print(\"\\nList from tensor:\", list_from_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking NumPy Arrays vs PyTorch Tensors (CPU and GPU Performance)\n",
    "\n",
    "This demonstration compares the performance of matrix multiplication using:\n",
    "\n",
    "1. NumPy (CPU): NumPy arrays are restricted to CPU computations.\n",
    "\n",
    "2. PyTorch (CPU): PyTorch Tensors, similar to NumPy arrays, are used on the CPU.\n",
    "\n",
    "3. PyTorch (GPU): PyTorch Tensors, when moved to the GPU, utilize hardware acceleration for faster computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by NumPy (CPU): 19.4701 seconds\n",
      "Time taken by PyTorch (CPU): 26.7916 seconds\n",
      "Time taken by PyTorch (GPU): 1.6697 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Define matrix sizes for the benchmark\n",
    "N = 10000\n",
    "\n",
    "# 1. NumPy (CPU)\n",
    "start_time = time.time()\n",
    "a_np = np.random.rand(N, N)\n",
    "b_np = np.random.rand(N, N)\n",
    "result_np = np.dot(a_np, b_np)\n",
    "time_numpy_cpu = time.time() - start_time\n",
    "print(f\"Time taken by NumPy (CPU): {time_numpy_cpu:.4f} seconds\")\n",
    "\n",
    "# 2. PyTorch (CPU)\n",
    "start_time = time.time()\n",
    "a_torch_cpu = torch.rand(N, N)\n",
    "b_torch_cpu = torch.rand(N, N)\n",
    "result_torch_cpu = torch.mm(a_torch_cpu, b_torch_cpu)\n",
    "time_torch_cpu = time.time() - start_time\n",
    "print(f\"Time taken by PyTorch (CPU): {time_torch_cpu:.4f} seconds\")\n",
    "\n",
    "# 3. PyTorch (GPU)\n",
    "if torch.cuda.is_available():\n",
    "    start_time = time.time()\n",
    "    a_torch_gpu = torch.rand(N, N, device='cuda')\n",
    "    b_torch_gpu = torch.rand(N, N, device='cuda')\n",
    "    result_torch_gpu = torch.mm(a_torch_gpu, b_torch_gpu)\n",
    "    torch.cuda.synchronize()  # Wait for all operations to finish on GPU\n",
    "    time_torch_gpu = time.time() - start_time\n",
    "    print(f\"Time taken by PyTorch (GPU): {time_torch_gpu:.4f} seconds\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping Tensors in PyTorch\n",
    "\n",
    "sneak peek to the future, In deep learning, especially in image classification models, it's common to reshape tensors while preserving their contents and number of elements. This is often necessary when transitioning from a convolutional layer to a linear (fully connected) layer.\n",
    "\n",
    "so let's learn how to control reshape a tensor using `reshape()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 20, 20])\n",
      "torch.Size([2400])\n"
     ]
    }
   ],
   "source": [
    "# Generate a 3D tensor with random values of shape (6, 20, 20)\n",
    "output3d = torch.rand(6, 20, 20)\n",
    "print(output3d.shape)  # Print the shape of the 3D tensor\n",
    "\n",
    "# Reshape the 3D tensor into a 1D tensor\n",
    "input1d = output3d.reshape(6 * 20 * 20)\n",
    "print(input1d.shape)  # Print the shape of the 1D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor\n",
    "import torch\n",
    "x = torch.arange(1., 8.)\n",
    "x, x.shape\n",
    "\n",
    "# Add an extra dimension\n",
    "x_reshaped = x.reshape(1, 7)\n",
    "x_reshaped, x_reshaped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Stacking\n",
    "\n",
    "If we wanted to stack our new tensor on top of itself five times, we could do so with `torch.stack()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4., 5., 6., 7.],\n",
       "        [1., 2., 3., 4., 5., 6., 7.],\n",
       "        [1., 2., 3., 4., 5., 6., 7.],\n",
       "        [1., 2., 3., 4., 5., 6., 7.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack tensors on top of each other\n",
    "x_stacked = torch.stack([x, x, x, x], dim=0) # try changing dim to dim=1 and see what happens\n",
    "x_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Squeezing\n",
    "In PyTorch, tensor squeezing refers to the process of removing dimensions of size 1 from a tensor's shape. This is done using the `torch.squeeze()` function. It is useful when you want to eliminate redundant dimensions that do not add value to the tensor's data.\n",
    "\n",
    "For example, if you have a tensor with the shape `(1, 3, 1, 4)`, where there are dimensions with size 1, `squeeze()` will remove these dimensions, resulting in a shape of `(3, 4)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1, 4])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor with extra dimensions of size 1\n",
    "tensor = torch.rand(1, 3, 1, 4)\n",
    "print(tensor.shape)  # Output: torch.Size([1, 3, 1, 4])\n",
    "\n",
    "# Apply squeeze\n",
    "squeezed_tensor = torch.squeeze(tensor)\n",
    "print(squeezed_tensor.shape)  # Output: torch.Size([3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsqueezing\n",
    "\n",
    "To add a dimension of size 1, you can use the `torch.unsqueeze()` function. This is the reverse operation of squeezing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor shape: torch.Size([3, 4])\n",
      "Shape after unsqueeze at dim 0: torch.Size([1, 3, 4])\n",
      "Shape after unsqueeze at dim 2: torch.Size([3, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor with shape (3, 4)\n",
    "tensor = torch.rand(3, 4)\n",
    "print(f\"Original tensor shape: {tensor.shape}\")  # Output: torch.Size([3, 4])\n",
    "\n",
    "# Apply unsqueeze to add a dimension at position 0 (before the first dimension)\n",
    "unsqueezed_tensor_0 = torch.unsqueeze(tensor, 0)\n",
    "print(f\"Shape after unsqueeze at dim 0: {unsqueezed_tensor_0.shape}\")  # Output: torch.Size([1, 3, 4])\n",
    "\n",
    "# Apply unsqueeze to add a dimension at position 2 (between dimensions 1 and 2)\n",
    "unsqueezed_tensor_2 = torch.unsqueeze(tensor, 2)\n",
    "print(f\"Shape after unsqueeze at dim 2: {unsqueezed_tensor_2.shape}\")  # Output: torch.Size([3, 1, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Happy Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
